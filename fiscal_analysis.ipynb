{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiscal Report Analysis\n",
    "\n",
    "This notebook demonstrates how to load a PDF, chunk the text, generate embeddings, and use a custom retriever to answer questions about the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up the Environment\n",
    "\n",
    "Here, we import all the required libraries and functions. We also load environment variables, which securely store sensitive information like API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import getpass\n",
    "import yaml\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "from IPython.display import display, Markdown, Code\n",
    "from air import AIRefinery, DistillerClient, login\n",
    "from air.utils import async_print\n",
    "from library.data_processing import extract_text_with_pdfplumber, extract_tables_with_pdfplumber, chunk_text, generate_document_embeddings, cache_embeddings, load_cached_embeddings\n",
    "from library.vector_store import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Securely Load Credentials\n",
    "\n",
    "We'll check if your AI Refinery credentials are saved in a `.env` file. If not, you'll be prompted to enter them, and we'll save them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "env_path = find_dotenv()\n",
    "if not env_path:\n",
    "    with open(\".env\", \"w\") as f:\n",
    "        pass\n",
    "    env_path = find_dotenv()\n",
    "\n",
    "account = os.getenv(\"ACCOUNT\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "if not account:\n",
    "    account = getpass.getpass(\"Enter your AI Refinery Account ID: \")\n",
    "    set_key(env_path, \"ACCOUNT\", account)\n",
    "\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass(\"Enter your AI Refinery API Key: \")\n",
    "    set_key(env_path, \"API_KEY\", api_key)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "print(\"Credentials loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Processing the Document\n",
    "\n",
    "We take a PDF fiscal report, read its text and tables, and break it down into smaller, more manageable chunks. We then convert these chunks into a numerical format (called embeddings) that our AI can understand. This process is saved (cached) so we don't have to repeat it every time we run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_CACHE_PATH = \"embeddings.pickle\"\n",
    "PDF_PATH = \"data/acn-third-quarter-fiscal-2025-earnings-release.pdf\"\n",
    "\n",
    "auth = login(\n",
    "    account=str(os.getenv(\"ACCOUNT\")),\n",
    "    api_key=str(os.getenv(\"API_KEY\")),\n",
    ")\n",
    "base_url = os.getenv(\"AIREFINERY_ADDRESS\", \"\")\n",
    "air_client = AIRefinery(**auth.openai(base_url=base_url))\n",
    "embedding_client = air_client.embeddings\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_CACHE_PATH):\n",
    "    cached_data = load_cached_embeddings(EMBEDDINGS_CACHE_PATH)\n",
    "    documents_from_pdf = cached_data[\"documents\"]\n",
    "    document_vectors = cached_data[\"vectors\"]\n",
    "else:\n",
    "    # Extract both text and tables using pdfplumber for better results\n",
    "    plain_text = extract_text_with_pdfplumber(PDF_PATH)\n",
    "    table_html = extract_tables_with_pdfplumber(PDF_PATH)\n",
    "    combined_content = plain_text + \"\\n\\n--- Extracted Tables ---\\n\" + table_html\n",
    "    \n",
    "    documents_from_pdf = chunk_text(combined_content)\n",
    "    document_vectors = generate_document_embeddings(documents_from_pdf, embedding_client)\n",
    "    if documents_from_pdf and document_vectors:\n",
    "        cache_embeddings(documents_from_pdf, document_vectors, EMBEDDINGS_CACHE_PATH)\n",
    "\n",
    "vector_store = InMemoryVectorStore(documents_from_pdf, document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Why Do We Chunk Documents?\n",
    "\n",
    "Large Language Models (LLMs) have a limited context window, meaning they can only process a certain amount of text at once. To work with large documents, we break them into smaller, overlapping 'chunks.' This ensures that the model has enough context to find relevant information without being overwhelmed. Below, you can see the first few chunks created from our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    chunk_id = f\"chunk_{i}\"\n",
    "    if chunk_id in documents_from_pdf:\n",
    "        print(f\"--- {chunk_id.upper()} ---\")\n",
    "        print(documents_from_pdf[chunk_id]['text'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating a Custom Search Tool\n",
    "\n",
    "This step defines our specialized search function. When we ask a question, this tool converts the question into the same numerical format as our document chunks. It then searches through the chunks to find the most relevant pieces of information to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_document_result(doc_id, doc, source_weight=1, retriever_name=\"\"):\n",
    "    base_score = doc.get(\"score\", 0.0)\n",
    "    final_score = float(base_score * source_weight)\n",
    "    content_text = doc.get(\"content\", {}).get(\"text\", \"\")\n",
    "    formatted_result = f\"Source: {retriever_name}\\nID: {doc_id}\\nContent: {content_text[:500]}...\"\n",
    "    return {\"result\": formatted_result, \"score\": final_score}\n",
    "\n",
    "async def custom_in_memory_vector_search(query: str):\n",
    "    print(f\"Received query for vector search: '{query}'\")\n",
    "    response = embedding_client.create(\n",
    "        input=[query],\n",
    "        model=\"nvidia/nv-embedqa-mistral-7b-v2\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "    query_vector = np.array(response.data[0].embedding, dtype=np.float32).reshape(1, -1)\n",
    "    \n",
    "    print(\"Searching for relevant documents in the vector store...\")\n",
    "    documents = vector_store.search(query_vector)\n",
    "\n",
    "    if not documents:\n",
    "        return [{\"result\": \"There is no relevant document from the PDF.\", \"score\": 0}]\n",
    "\n",
    "    results = [\n",
    "        _format_document_result(\n",
    "            doc[\"id\"], doc, source_weight=1, retriever_name=\"PDF-NumPy-Retriever\"\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Initialize the Research Agent\n",
    "\n",
    "Now, we set up the first AI agent that will use our custom search tool. We create a project for simple questions (`DocumentSearch`). This setup only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller_client = DistillerClient(base_url=base_url)\n",
    "uuid = os.getenv(\"UUID\", \"test_user_refactored\")\n",
    "research_config_path = \"custom_vector_search.yaml\"\n",
    "\n",
    "# Create project for the simple Research Agent\n",
    "research_project = \"DocumentSearch\"\n",
    "distiller_client.create_project(\n",
    "    config_path=research_config_path, project=research_project\n",
    ")\n",
    "\n",
    "# Define the executor dictionary for the agent\n",
    "research_executor_dict = {\n",
    "    \"Research Agent\": {\n",
    "        \"Fiscal Reports Database\": custom_in_memory_vector_search,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Research Agent project created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Visualize the Research Agent Workflow\n",
    "\n",
    "Here we can see a simple diagram of our Research Agent. The `Distiller Orchestrator` takes our query and passes it to the `Research Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(research_config_path, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "orchestrator_name = \"Distiller Orchestrator\"\n",
    "agent_name = config_data.get('utility_agents', [{}])[0].get('agent_name', 'Research Agent')\n",
    "\n",
    "mermaid_code = f\"\"\"graph TD\n",
    "    A[User Query] --> O(({orchestrator_name}));\n",
    "    O --> B[{agent_name}];\n",
    "\"\"\"\n",
    "display(Markdown(\"```mermaid\\n\" + mermaid_code + \"\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Answering Questions with the Research Agent\n",
    "\n",
    "We can now use our 'Research Agent' and the custom vector database to ask a specific question. The agent uses our custom search tool to find the answer in the document. **You can change the `query` text in the cell below and rerun it to ask a new question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_simple_question(query: str):\n",
    "    \"\"\"Runs a query using the Research Agent.\"\"\"\n",
    "    async with distiller_client(\n",
    "        project=research_project,\n",
    "        uuid=uuid,\n",
    "        executor_dict=research_executor_dict,\n",
    "    ) as dc:\n",
    "        print(f\"----\\nQuery: {query}\")\n",
    "        responses = await dc.query(query=query)\n",
    "        async for response in responses:\n",
    "            await async_print(f\"Response: {response['content']}\")\n",
    "\n",
    "# Define your query here\n",
    "simple_query = \"what did julie say in the report?\"\n",
    "await ask_simple_question(simple_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Initialize the Flow Super Agent\n",
    "\n",
    "For more complex questions, we'll set up a more advanced 'Flow Super Agent'. This agent can perform multi-step reasoning to provide a comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config_path = \"flow_super_agent.yaml\"\n",
    "\n",
    "# Create project for the Flow Super Agent\n",
    "flow_project = \"FinancialAnalysisFlow\"\n",
    "distiller_client.create_project(\n",
    "    config_path=flow_config_path, project=flow_project\n",
    ")\n",
    "\n",
    "# Define the executor dictionary for the agent\n",
    "flow_executor_dict = {\n",
    "    \"Fiscal Report Researcher\": {\n",
    "        \"Fiscal Reports Database\": custom_in_memory_vector_search,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Flow Super Agent project created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Visualize the Flow Super Agent Workflow\n",
    "\n",
    "This diagram shows the more complex workflow of our `FlowSuperAgent`. The `Distiller Orchestrator` initiates the `FlowSuperAgent`, which in turn orchestrates multiple specialized agents to build a final, comprehensive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(flow_config_path, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "orchestrator_name = \"Distiller Orchestrator\"\n",
    "super_agent_config = config_data.get('super_agents', [{}])[0]\n",
    "super_agent_name = super_agent_config.get('agent_name', 'Super Agent')\n",
    "agent_flow = super_agent_config.get('config', {}).get('agent_list', [])\n",
    "entry_agent = agent_flow[0]['agent_name'] if agent_flow else ''\n",
    "\n",
    "mermaid_lines = [\"graph TD\"]\n",
    "mermaid_lines.append(f\"    O(({orchestrator_name})) --> S[{super_agent_name}]\")\n",
    "mermaid_lines.append(f\"    subgraph {super_agent_name}\")\n",
    "mermaid_lines.append(\"        direction LR\")\n",
    "\n",
    "for agent_step in agent_flow:\n",
    "    from_agent = agent_step['agent_name']\n",
    "    if 'next_step' in agent_step:\n",
    "        for to_agent in agent_step['next_step']:\n",
    "            mermaid_lines.append(f\"        {from_agent.replace(' ', '_')}[{from_agent}] --> {to_agent.replace(' ', '_')}[{to_agent}]\")\n",
    "    else:\n",
    "        mermaid_lines.append(f\"        {from_agent.replace(' ', '_')}[{from_agent}]\")\n",
    "\n",
    "mermaid_lines.append(\"    end\")\n",
    "mermaid_code = \"\\n\".join(mermaid_lines)\n",
    "\n",
    "display(Markdown(\"```mermaid\\n\" + mermaid_code + \"\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Performing a Deeper Analysis\n",
    "\n",
    "Now we can use the 'Flow Super Agent' for questions that require reasoning and combining information. **You can change the `query` text in the cell below and rerun it to request a new analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def perform_comparative_analysis(query: str):\n",
    "    \"\"\"Runs a query using the Flow Super Agent.\"\"\"\n",
    "    async with distiller_client(\n",
    "        project=flow_project,\n",
    "        uuid=uuid,\n",
    "        executor_dict=flow_executor_dict,\n",
    "    ) as dc:\n",
    "        print(f\"----\\nQuery: {query}\")\n",
    "        responses = await dc.query(query=query)\n",
    "        async for response in responses:\n",
    "            await async_print(f\"Role: {response.get('role', 'System')}\\nContent: {response.get('content', '')}\\n\")\n",
    "\n",
    "# Define your query here\n",
    "analysis_query = \"Provide a comparative analysis of the attached fiscal report.\"\n",
    "await perform_comparative_analysis(analysis_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Customize the Flow Super Agent\n",
    "\n",
    "Now it's your turn to experiment! You can customize the behavior of the `FlowSuperAgent` by editing the `flow_super_agent.yaml` file.\n",
    "\n",
    "To do this, use the file browser on the left panel of your Jupyter environment to find and open `flow_super_agent.yaml`. After you save your changes, you can rerun this notebook from **Step 9** to see your new workflow in action.\n",
    "\n",
    "Here are a few ideas to get you started:\n",
    "\n",
    "1.  **Change the Goal:** Modify the `goal` in the `Financial Analyst` super agent's config to focus on a different aspect, like \"risk assessment\" or \"growth opportunities.\"\n",
    "2.  **Add a New Agent:** Introduce a new `utility_agent`, like the [CriticalThinker Agent](https://sdk.airefinery.accenture.com/distiller/agent-library/utility_agents/criticalthinker/), to challenge the assumptions of the `Comparative Analysis Writer` before the final report is generated. You can find a full list of available agents in the [Agent Library](https://sdk.airefinery.accenture.com/distiller/agent-library/).\n",
    "3.  **Modify the Flow:** Change the `next_step` for the existing agents. For example, have the `Market Analyst` also feed information directly to the `Competitor Analyst`.\n",
    "4.  **Customize Leading Questions:** Edit the `leading_questions` for the `Comparative Analysis Writer` to change the structure and focus of the final report. For instance, add a question about the company's sustainability initiatives.\n",
    "5.  **Simplify the Flow:** For a quicker analysis, remove one of the agents, like the `Competitor Analyst`, and adjust the `next_step` pointers accordingly to create a more direct path to the `Comparative Analysis Writer`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
