{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiscal Report Analysis with a Custom NumPy Vector Store\n",
    "\n",
    "This notebook demonstrates how to build a custom, in-memory vector search system to analyze fiscal reports. We'll load a PDF, chunk the text, generate embeddings, and use a custom retriever to answer questions about the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install airefinery-sdk python-dotenv numpy pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from air import AIRefinery, DistillerClient, login\n",
    "from air.utils import async_print\n",
    "from library.data_processing import extract_text_with_pdfplumber, extract_tables_with_pdfplumber, chunk_text, generate_document_embeddings, cache_embeddings, load_cached_embeddings\n",
    "from library.vector_store import InMemoryVectorStore\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Process the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_CACHE_PATH = \"embeddings.pickle\"\n",
    "PDF_PATH = \"data/acn-third-quarter-fiscal-2025-earnings-release.pdf\"\n",
    "\n",
    "auth = login(\n",
    "    account=str(os.getenv(\"ACCOUNT\")),\n",
    "    api_key=str(os.getenv(\"API_KEY\")),\n",
    ")\n",
    "base_url = os.getenv(\"AIREFINERY_ADDRESS\", \"\")\n",
    "air_client = AIRefinery(**auth.openai(base_url=base_url))\n",
    "embedding_client = air_client.embeddings\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_CACHE_PATH):\n",
    "    cached_data = load_cached_embeddings(EMBEDDINGS_CACHE_PATH)\n",
    "    documents_from_pdf = cached_data[\"documents\"]\n",
    "    document_vectors = cached_data[\"vectors\"]\n",
    "else:\n",
    "    # Extract both text and tables using pdfplumber for better results\n",
    "    plain_text = extract_text_with_pdfplumber(PDF_PATH)\n",
    "    table_html = extract_tables_with_pdfplumber(PDF_PATH)\n",
    "    combined_content = plain_text + \"\\n\\n--- Extracted Tables ---\\n\" + table_html\n",
    "    \n",
    "    documents_from_pdf = chunk_text(combined_content)\n",
    "    document_vectors = generate_document_embeddings(documents_from_pdf, embedding_client)\n",
    "    if documents_from_pdf and document_vectors:\n",
    "        cache_embeddings(documents_from_pdf, document_vectors, EMBEDDINGS_CACHE_PATH)\n",
    "\n",
    "vector_store = InMemoryVectorStore(documents_from_pdf, document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Custom Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_document_result(doc_id, doc, source_weight=1, retriever_name=\"\"):\n",
    "    base_score = doc.get(\"score\", 0.0)\n",
    "    final_score = float(base_score * source_weight)\n",
    "    content_text = doc.get(\"content\", {}).get(\"text\", \"\")\n",
    "    formatted_result = f\"Source: {retriever_name}\\nID: {doc_id}\\nContent: {content_text[:500]}...\"\n",
    "    return {\"result\": formatted_result, \"score\": final_score}\n",
    "\n",
    "async def custom_in_memory_vector_search(query: str):\n",
    "    print(f\"Received query for vector search: '{query}'\")\n",
    "    response = embedding_client.create(\n",
    "        input=[query],\n",
    "        model=\"nvidia/nv-embedqa-mistral-7b-v2\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "    query_vector = np.array(response.data[0].embedding, dtype=np.float32).reshape(1, -1)\n",
    "    \n",
    "    print(\"Searching for relevant documents in the vector store...\")\n",
    "    documents = vector_store.search(query_vector)\n",
    "\n",
    "    if not documents:\n",
    "        return [{\"result\": \"There is no relevant document from the PDF.\", \"score\": 0}]\n",
    "\n",
    "    results = [\n",
    "        _format_document_result(\n",
    "            doc[\"id\"], doc, source_weight=1, retriever_name=\"PDF-NumPy-Retriever\"\n",
    "        )\n",
    "        for doc in documents\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run a Simple Query with the Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_simple_query():\n",
    "    distiller_client = DistillerClient(base_url=base_url)\n",
    "    project = \"DocumentSearch\"\n",
    "    uuid = os.getenv(\"UUID\", \"test_user_simple\")\n",
    "\n",
    "    distiller_client.create_project(\n",
    "        config_path=\"custom_vector_search.yaml\", project=project\n",
    "    )\n",
    "\n",
    "    executor_dict = {\n",
    "        \"Research Agent\": {\n",
    "            \"Fiscal Reports Database\": custom_in_memory_vector_search,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    async with distiller_client(\n",
    "        project=project,\n",
    "        uuid=uuid,\n",
    "        executor_dict=executor_dict,\n",
    "    ) as dc:\n",
    "        query = \"how much generative ai bookings were there?\"\n",
    "        responses = await dc.query(query=query)\n",
    "        print(f\"----\\nQuery: {query}\")\n",
    "        async for response in responses:\n",
    "            await async_print(f\"Response: {response['content']}\")\n",
    "\n",
    "await run_simple_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run a Comparative Analysis with the Flow Super Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_flow_super_agent():\n",
    "    distiller_client = DistillerClient(base_url=base_url)\n",
    "    project = \"FinancialAnalysisFlow\"\n",
    "    uuid = os.getenv(\"UUID\", \"test_user_flow\")\n",
    "\n",
    "    distiller_client.create_project(\n",
    "        config_path=\"flow_super_agent.yaml\", project=project\n",
    "    )\n",
    "\n",
    "    executor_dict = {\n",
    "        \"Fiscal Report Researcher\": {\n",
    "            \"Fiscal Reports Database\": custom_in_memory_vector_search,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    async with distiller_client(\n",
    "        project=project,\n",
    "        uuid=uuid,\n",
    "        executor_dict=executor_dict,\n",
    "    ) as dc:\n",
    "        query = \"Provide a comparative analysis of the attached fiscal report.\"\n",
    "        responses = await dc.query(query=query)\n",
    "        print(f\"----\\nQuery: {query}\")\n",
    "        async for response in responses:\n",
    "            await async_print(f\"Role: {response.get('role', 'System')}\\nContent: {response.get('content', '')}\\n\")\n",
    "\n",
    "await run_flow_super_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
